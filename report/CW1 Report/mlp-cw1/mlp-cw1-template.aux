\relax 
\bibstyle{icml2017}
\@writefile{toc}{\contentsline {section}{\numberline {1}Introduction}{2}}
\newlabel{sec:intro}{{1}{2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.1}Neural Networks}{2}}
\newlabel{sec:subintro}{{1.1}{2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.2}MNIST Dataset}{2}}
\newlabel{sec:dataset}{{1.2}{2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.3}Default testing environment}{2}}
\newlabel{sec:testing_env}{{1.3}{2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.4}Activation Functions}{2}}
\newlabel{sec:activation_funct}{{1.4}{2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.5}Deep Neural Networks}{3}}
\newlabel{sec:deep_nets}{{1.5}{3}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.6}Weight Initialisation}{3}}
\newlabel{sec:initialisation}{{1.6}{3}}
\@writefile{toc}{\contentsline {section}{\numberline {2}Activation functions and their experimental comparison}{3}}
\newlabel{sec:actfn}{{2}{3}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1}Sigmoid}{3}}
\newlabel{sec:sigmoid}{{2.1}{3}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2}ReLU}{3}}
\newlabel{sec:relu}{{2.2}{3}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3}Leaky ReLU}{4}}
\newlabel{sec:lrelu}{{2.3}{4}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.4}ELU}{4}}
\newlabel{sec:elu}{{2.4}{4}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.5}SELU}{4}}
\newlabel{sec:selu}{{2.5}{4}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.6}Experimental Comparison of Activation Functions}{4}}
\@writefile{toc}{\contentsline {section}{\numberline {3}Deep neural network experiments}{4}}
\newlabel{sec:dnnexpts}{{3}{4}}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces Sigmoid activation function}}{5}}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces ReLU activation function}}{5}}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces Leaky ReLU activation function}}{5}}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces ELU activation function}}{5}}
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces SELU activation function}}{5}}
\@writefile{lof}{\contentsline {figure}{\numberline {6}{\ignorespaces 2-layer ELU Neural Network}}{5}}
\@writefile{lof}{\contentsline {figure}{\numberline {7}{\ignorespaces 5-layer ELU Neural Network}}{5}}
\@writefile{lof}{\contentsline {figure}{\numberline {8}{\ignorespaces 8-layer ELU Neural Network}}{5}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1}Analysis of the number of layers}{5}}
\bibdata{example-refs}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2}Fan-In Initialisation}{6}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3}Fan-In and Fan-Out Initialisation}{6}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.4}Gaussian Initialisation on SELU}{6}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.5}Analysis of initialisation schemes}{6}}
\@writefile{toc}{\contentsline {section}{\numberline {4}Conclusions}{6}}
\newlabel{sec:concl}{{4}{6}}
\@writefile{lof}{\contentsline {figure}{\numberline {10}{\ignorespaces Fan-In and Fan-Out Initialisation}}{7}}
\@writefile{lof}{\contentsline {figure}{\numberline {11}{\ignorespaces Gaussian intialisation on SELU}}{7}}
